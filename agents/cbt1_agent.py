import os, json, multiprocessing
from typing import AsyncGenerator, Literal, List, Optional
from pydantic import BaseModel
from llama_cpp import Llama

# âœ… ëª¨ë¸ ìºì‹œ
LLM_CBT1_INSTANCE = {}

def load_cbt1_model(model_path: str) -> Llama:
    global LLM_CBT1_INSTANCE
    if model_path not in LLM_CBT1_INSTANCE:
        print("ğŸš€ CBT1 ëª¨ë¸ ë¡œë”© ì¤‘...", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_CBT1_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_ctx=1024,
            n_threads=NUM_THREADS,
            n_batch=8,
            max_tokens=128,
            temperature=0.75,
            top_p=0.9,
            presence_penalty=1.0,
            frequency_penalty=0.8,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>"]
        )
        print("âœ… CBT1 ëª¨ë¸ ë¡œë”© ì™„ë£Œ:", model_path)
    return LLM_CBT1_INSTANCE[model_path]

# âœ… ìƒíƒœ ì •ì˜
class AgentState(BaseModel):
    stage: Literal["cbt1", "cbt2"]
    question: str
    response: str
    history: List[str]
    turn: int
    intro_shown: bool
    pending_response: Optional[str] = None

# âœ… í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ CBT1 ì‘ë‹µ
async def stream_cbt1_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    user_input = state.question.strip()
    history = state.history or []

    # âœ… ì¸íŠ¸ë¡œ ì¶œë ¥: ì²˜ìŒ ì§„ì… ì‹œ
    if not state.intro_shown:
        intro = "ì§€ê¸ˆë¶€í„° ìë™ì ìœ¼ë¡œ ë– ì˜¤ë¥´ëŠ” ìƒê°ì„ í•¨ê»˜ ì‚´í´ë³¼ê²Œìš”. í¸í•˜ê²Œ ë– ì˜¤ë¥¸ ìƒê°ì„ ë§ì”€í•´ ì£¼ì„¸ìš”."
        yield intro.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt1",
            "turn": 0,
            "response": intro,
            "question": "",
            "intro_shown": True,
            "history": history + [intro]
        }, ensure_ascii=False).encode("utf-8")
        return

    # âœ… ì…ë ¥ ë¯¸ë¹„ ì‹œ
    if not user_input:
        fallback = "ë– ì˜¤ë¥¸ ìƒê°ì´ë‚˜ ê°ì •ì´ ìˆë‹¤ë©´ í¸í•˜ê²Œ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”."
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt1",
            "turn": state.turn,
            "response": fallback,
            "question": "",
            "intro_shown": True,
            "history": history
        }, ensure_ascii=False).encode("utf-8")
        return

    try:
        llm = load_cbt1_model(model_path)

        system_prompt = (
            "ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì´ì„±ì ì¸ ì†Œí¬ë¼í…ŒìŠ¤ ìƒë‹´ìì•¼. ì‚¬ìš©ìì˜ ìë™ì‚¬ê³ ë¥¼ íƒìƒ‰í•´ì•¼ í•´.\n"
            "- ë§¤ë²ˆ ìƒˆë¡œìš´ ì‹œê°ìœ¼ë¡œ ì§ˆë¬¸ì„ ë˜ì ¸ì•¼ í•´.\n"
            "- ì§ˆë¬¸ì€ 1~2ë¬¸ì¥, ì¡´ëŒ“ë§ë¡œ ë§ˆë¬´ë¦¬í•´.\n"
            "- ê°ì •, ê·¼ê±°, ê²°ê³¼, ëŒ€ì•ˆì‚¬ê³ , ìƒê°ì˜ íŒ¨í„´ì„ ë‹¤ì–‘í•˜ê²Œ ìœ ë„í•´.\n"
            "- ì˜ˆì‹œ: "
            "'ê·¸ ìƒê°ì´ ë“¤ì—ˆì„ ë•Œ ì–´ë–¤ ê°ì •ì´ ê°€ì¥ ì»¸ë‚˜ìš”?', "
            "'ê·¸ ìƒê°ì´ ì‚¬ì‹¤ì´ë¼ê³  ëŠë‚€ ì´ìœ ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?', "
            "'ë¹„ìŠ·í•œ ìƒí™©ì—ì„œ ëŠ˜ ì´ëŸ° ìƒê°ì´ ë“œì‹œë‚˜ìš”?', "
            "'ê·¸ ìƒê°ì„ ê³„ì† ë¯¿ìœ¼ë©´ ì–´ë–¤ ê²°ê³¼ê°€ ìƒê¸¸ê¹Œìš”?', "
            "'ë‹¤ë¥¸ ì‹œê°ì—ì„œ ë³´ë©´ ì–´ë–¤ í•´ì„ì´ ê°€ëŠ¥í• ê¹Œìš”?', "
            "'ì¹œí•œ ì¹œêµ¬ê°€ ê°™ì€ ë§ì„ í–ˆë‹¤ë©´ ë­ë¼ê³  ë‹µí–ˆì„ ê²ƒ ê°™ë‚˜ìš”?'"
        )

        messages = [{"role": "system", "content": system_prompt}]
        for i in range(0, len(history), 2):
            if i + 1 < len(history):
                messages.append({"role": "user", "content": history[i]})
                messages.append({"role": "assistant", "content": history[i + 1]})
        messages.append({"role": "user", "content": user_input})

        full_response, first_token_sent = "", False
        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            token = chunk["choices"][0]["delta"].get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip()
        if not reply:
            reply = "ë„¤, ê·¸ ìƒê°ì„ ë” ìì„¸íˆ ë“¤ì—¬ë‹¤ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?"

        # âœ… í„´ ì¦ê°€ ë° ì „í™˜ ì²˜ë¦¬
        next_turn = state.turn + 1
        next_stage = "cbt2" if next_turn >= 5 else "cbt1"
        if next_stage == "cbt2":
            reply += "\n\nğŸ‘ ì˜í•˜ì…¨ì–´ìš”. ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ë– ì˜¤ë¥¸ ìƒê°ì„ ì¬êµ¬ì„±í•´ë³´ëŠ” ì—°ìŠµì„ í•´ë³¼ê²Œìš”."

        updated_history = history.copy()
        if not (len(updated_history) >= 2 and updated_history[-2] == user_input and updated_history[-1] == reply):
            updated_history.extend([user_input, reply])

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "turn": 0 if next_stage == "cbt2" else next_turn,
            "response": reply,
            "question": "",
            "intro_shown": True,
            "history": updated_history
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        err_msg = f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(err_msg, flush=True)
        fallback = "ì£„ì†¡í•´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt1",
            "turn": state.turn,
            "response": fallback,
            "question": "",
            "intro_shown": True,
            "history": history
        }, ensure_ascii=False).encode("utf-8")
