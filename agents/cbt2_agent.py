import os, json, multiprocessing, difflib
from typing import AsyncGenerator, List
from pydantic import BaseModel
from llama_cpp import Llama

LLM_CBT2_INSTANCE = {}

def load_cbt2_model(model_path: str) -> Llama:
    global LLM_CBT2_INSTANCE
    if model_path not in LLM_CBT2_INSTANCE:
        print("ğŸš€ CBT2 ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_CBT2_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_ctx=1024,
            n_threads=NUM_THREADS,
            n_batch=4,
            max_tokens=128,
            temperature=0.6,
            top_p=0.85,
            repeat_penalty=1.1,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>", "\n\n"]
        )
        print("âœ… CBT2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", flush=True)
    return LLM_CBT2_INSTANCE[model_path]

class AgentState(BaseModel):
    question: str
    response: str
    history: List[str]
    turn: int
    intro_shown: bool

def get_cbt2_prompt() -> str:
    return (
        "ë„ˆëŠ” ì¸ì§€ ì¬êµ¬ì¡°í™”ë¥¼ ë„ì™€ì£¼ëŠ” ì „ë¬¸ CBT ìƒë‹´ìì•¼.\n"
        "- ë°˜ë“œì‹œ í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ í•˜ì„¸ìš”. ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "- ìë™ì‚¬ê³ ì— ë„ì „í•˜ê³  ì™œê³¡ëœ ì‚¬ê³ ë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ ê´€ì ì˜ ì§ˆë¬¸ì„ í•´.\n"
        "- ì£¼ì œë¥¼ ëŒì•„ê°€ë©° ì§ˆë¬¸í•´: ê°ì •, ì‚¬ì‹¤ ì—¬ë¶€, ëŒ€ì•ˆ í•´ì„, ê°€ì¹˜ íŒë‹¨, ì‹ ë… ê²€í† , íƒ€ì¸ì˜ ê´€ì , ì¥ê¸°ì  ì˜í–¥, ë°˜ë³µëœ íŒ¨í„´, ê¸ì •ì  ê°€ëŠ¥ì„± ë“±\n"
        "- ì§ˆë¬¸ì€ ì¡´ëŒ“ë§ë¡œ ì§§ê³  ë”°ëœ»í•˜ê²Œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”.\n"
        "- ê°™ì€ êµ¬ì¡°ì˜ ì§ˆë¬¸ì€ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.\n"
        "- ì˜ˆì‹œ:\n"
        "  - 'ê·¸ ìƒê°ì€ ì–´ë–¤ ê·¼ê±°ì—ì„œ ë¹„ë¡¯ëœ ê±¸ê¹Œìš”?'\n"
        "  - 'í˜¹ì‹œ ì´ì „ì—ë„ ë¹„ìŠ·í•œ ìƒí™©ì„ ê²½í—˜í•˜ì‹  ì  ìˆìœ¼ì‹ ê°€ìš”?'\n"
        "  - 'ê·¸ ìƒê°ì´ ì§€ì†ëœë‹¤ë©´ ì–´ë–¤ ì¥ê¸°ì ì¸ ì˜í–¥ì´ ìƒê¸¸ ìˆ˜ ìˆì„ê¹Œìš”?'\n"
        "  - 'ë‹¤ë¥¸ ì‹œê°ì—ì„œ ë³´ë©´ ì´ ìƒí™©ì„ ì–´ë–»ê²Œ ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?'\n"
        "  - 'ì´ ìƒê°ì´ ì§€ê¸ˆì˜ ê°ì •ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ê³  ìˆì„ê¹Œìš”?'"
    )

async def stream_cbt2_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    user_input = state.question.strip()
    
    # âœ… ìµœì´ˆ ì§„ì… ì‹œ ì¸íŠ¸ë¡œ ì¶œë ¥
    if not state.intro_shown:
        intro = "ì´ì œë¶€í„°ëŠ” ë– ì˜¤ë¥¸ ìƒê°ì„ ë‹¤ì–‘í•œ ì‹œê°ì—ì„œ ë‹¤ì‹œ ë°”ë¼ë³´ëŠ” ì—°ìŠµì„ í•´ë³¼ ê±°ì˜ˆìš”. ì²œì²œíˆ ìƒê°ì„ ë‚˜ëˆ  ì£¼ì„¸ìš”."
        yield intro.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt2",
            "response": intro,
            "history": state.history + [intro],
            "turn": 0,
            "intro_shown": True
        }, ensure_ascii=False).encode("utf-8")
        return

    if not user_input:
        fallback = "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt2",
            "response": fallback,
            "history": state.history + [user_input, fallback],
            "turn": state.turn + 1,
            "intro_shown": True
        }, ensure_ascii=False).encode("utf-8")
        return

    try:
        llm = load_cbt2_model(model_path)

        messages = [{"role": "system", "content": get_cbt2_prompt()}]
        for i in range(max(0, len(state.history) - 10), len(state.history), 2):
            messages.append({"role": "user", "content": state.history[i]})
            if i + 1 < len(state.history):
                messages.append({"role": "assistant", "content": state.history[i + 1]})
        messages.append({"role": "user", "content": user_input})

        full_response, first_token_sent = "", False
        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            token = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip() or "ê´œì°®ìŠµë‹ˆë‹¤. ì²œì²œíˆ ìƒê°ì„ ì •ë¦¬í•´ ë§ì”€í•´ì£¼ì…”ë„ ë¼ìš”."

        # âœ… ìœ ì‚¬í•œ ì§ˆë¬¸ íšŒí”¼
        for past in state.history[-10:]:
            if isinstance(past, str):
                similarity = difflib.SequenceMatcher(None, reply[:30], past[:30]).ratio()
                if similarity > 0.85:
                    reply += " ì´ë²ˆì—” ì¡°ê¸ˆ ë‹¤ë¥¸ ë°©í–¥ì—ì„œ ìƒê°í•´ë³¼ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ë“œë ¸ì–´ìš”."
                    break

        # âœ… í„´ ìˆ˜ ê¸°ì¤€ ì „í™˜
        next_turn = state.turn + 1
        next_stage = "cbt3" if next_turn >= 5 else "cbt2"

        if next_stage == "cbt3":
            reply += "\n\nğŸ§  ì´ì œ ìƒê°ì„ ì¬êµ¬ì„±í•˜ëŠ” CBT3 ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆì–´ìš”."

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "response": reply,
            "history": state.history + [user_input, reply],
            "turn": 0 if next_stage == "cbt3" else next_turn,
            "intro_shown": True
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        print(f"âš ï¸ CBT2 ì˜¤ë¥˜: {e}", flush=True)
        fallback = "ì£„ì†¡í•´ìš”. ë‹¤ì‹œ í•œ ë²ˆ ì´ì•¼ê¸°í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt2",
            "response": fallback,
            "history": state.history + [user_input],
            "turn": state.turn + 1,
            "intro_shown": True
        }, ensure_ascii=False).encode("utf-8")
