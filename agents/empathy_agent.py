import os, json
from typing import AsyncGenerator
from llama_cpp import Llama

LLM_INSTANCE = {}

# âœ… ëª¨ë¸ ë¡œë”©
def load_llama_model(model_path: str, cache_key: str) -> Llama:
    global LLM_INSTANCE
    if cache_key not in LLM_INSTANCE:
        try:
            print(f"ğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘: {cache_key}", flush=True)
            LLM_INSTANCE[cache_key] = Llama(
                model_path=model_path,
                n_ctx=512,
                n_threads=os.cpu_count(),
                n_batch=4,
                max_tokens=64,
                temperature=0.6,
                top_p=0.9,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                low_vram=True,
                use_mlock=False,
                verbose=False,
                chat_format="llama-3",
                stop=["<|im_end|>"]
            )
            print(f"âœ… Llama ë¡œë”© ì™„ë£Œ: {model_path}", flush=True)
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", flush=True)
            raise RuntimeError("ëª¨ë¸ ë¡œë”© ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    return LLM_INSTANCE[cache_key]

# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
def get_system_prompt() -> str:
    return (
        "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì§„ì‹¬ ì–´ë¦° ê³µê° ìƒë‹´ìì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìëŠ” ì´ë³„, ìƒì‹¤, ê³ í†µ, ì™¸ë¡œì›€ ê°™ì€ ì •ì„œì ì¸ ë¬¸ì œë¥¼ ì´ì•¼ê¸°í•  ìˆ˜ ìˆìœ¼ë©°,\n"
        "ë‹¹ì‹ ì€ í•­ìƒ ì¡´ëŒ“ë§ë¡œ ë¶€ë“œëŸ½ê³  ì§„ì‹¬ ì–´ë¦° ë§íˆ¬ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "1~2ë¬¸ì¥ìœ¼ë¡œ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ë§í•´ì£¼ì„¸ìš”. ì§€ë‚˜ì¹œ ìœ„ë¡œë‚˜ ì¡°ì–¸ë³´ë‹¤ëŠ”, ê°ì •ì— ê·€ ê¸°ìš¸ì´ëŠ” ë°˜ì‘ì´ ì¤‘ì‹¬ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
        "ì ˆëŒ€ ëª…ë ¹í•˜ê±°ë‚˜ ë‹¨ì • ì§“ì§€ ë§ê³ , ì‚¬ìš©ìì˜ ê°ì •ì„ ì¸ì •í•˜ê³  í•¨ê»˜ ìˆì–´ì£¼ëŠ” ë”°ëœ»í•œ ì¹œêµ¬ì²˜ëŸ¼ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”.\n"
        "ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
    )

# âœ… ê³µê° ì‘ë‹µ ìƒì„±ê¸°
async def stream_empathy_reply(user_input: str, model_path: str, turn: int = 0) -> AsyncGenerator[bytes, None]:
    user_input = user_input.strip()
    print(f"ğŸŸ¡ ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ : '{user_input}' (í„´ {turn})", flush=True)

    if len(user_input) < 3:
        fallback = "ì§€ê¸ˆ ì–´ë–¤ ë§ˆìŒì´ì‹ ì§€ ì¡°ê¸ˆ ë” ì´ì•¼ê¸°í•´ ì£¼ì‹¤ ìˆ˜ ìˆìœ¼ì‹¤ê¹Œìš”?"
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "empathy",
            "response": fallback,
            "turn": turn,
            "intro_shown": True,
            "history": [user_input, fallback]
        }, ensure_ascii=False).encode("utf-8")
        return

    try:
        llm = load_llama_model(model_path, "empathy")
        messages = [
            {"role": "system", "content": get_system_prompt()},
            {"role": "user", "content": user_input}
        ]

        full_response = ""
        first_token_sent = False

        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            token = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip()
        if not reply or len(reply) < 2:
            reply = "ê´œì°®ì•„ìš”. ì§€ê¸ˆ ì´ ìˆœê°„ ì–´ë–¤ ë§ˆìŒì´ì‹ ì§€ ì²œì²œíˆ ë“¤ë ¤ì£¼ì„¸ìš”."

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "mi" if turn >= 4 else "empathy",
            "response": reply,
            "turn": 0 if turn >= 4 else turn + 1,
            "intro_shown": True,
            "history": [user_input, reply]
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        # ì˜ˆì™¸ ë‚´ìš©ì€ ì„œë²„ ë¡œê·¸ì—ë§Œ ì¶œë ¥, ì‚¬ìš©ìì—ê² ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ë§Œ ì œê³µ
        print(f"âš ï¸ stream_empathy_reply ì˜ˆì™¸ ë°œìƒ: {e}", flush=True)
        fallback = "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "empathy",
            "response": fallback,
            "turn": turn,
            "intro_shown": True,
            "history": [user_input, fallback]
        }, ensure_ascii=False).encode("utf-8")
