import os, json, multiprocessing, difflib, re
from typing import AsyncGenerator, Literal, List
from pydantic import BaseModel
from llama_cpp import Llama

# âœ… ëª¨ë¸ ìºì‹œ
LLM_CBT3_INSTANCE = {}

def load_cbt3_model(model_path: str) -> Llama:
    global LLM_CBT3_INSTANCE
    if model_path not in LLM_CBT3_INSTANCE:
        print("ğŸš€ CBT3 ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_CBT3_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_ctx=1024,
            n_threads=NUM_THREADS,
            n_batch=4,
            max_tokens=128,
            temperature=0.65,
            top_p=0.9,
            presence_penalty=1.0,
            frequency_penalty=0.8,
            repeat_penalty=1.1,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>", "\n\n"]
        )
        print(f"âœ… CBT3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_path}", flush=True)
    return LLM_CBT3_INSTANCE[model_path]

# âœ… ìƒíƒœ ëª¨ë¸ ì •ì˜
class AgentState(BaseModel):
    stage: Literal["cbt3", "end"]
    question: str
    response: str
    history: List[str]
    turn: int
    preset_questions: List[str] = []

# âœ… ì§ˆë¬¸ ì„¸íŠ¸ ìƒì„±
def generate_preset_questions(llm: Llama) -> List[str]:
    prompt = (
        "ë„ˆëŠ” ë”°ëœ»í•˜ê³  ë…¼ë¦¬ì ì¸ CBT ìƒë‹´ìì•¼. ì‚¬ìš©ìê°€ ì‹¤ì²œí•  ìˆ˜ ìˆë„ë¡ ì´ëŒ ìˆ˜ ìˆëŠ” ì—´ë¦° ì§ˆë¬¸ 5ê°œë¥¼ ì œì•ˆí•´ì¤˜. "
        "ì§ˆë¬¸ì€ ì§§ê³  ëª…í™•í•´ì•¼ í•´. ë‹¤ìŒ ì£¼ì œë¥¼ í™œìš©í•´ë„ ì¢‹ì•„: ë°©í•´ ìš”ì¸, ê°ì • ë³€í™”, ìŠµê´€ í˜•ì„±, í™˜ê²½ ì¡°ì •, í”¼ë“œë°± ì‹¤ì²œ."
    )
    result = llm.create_completion(prompt=prompt, max_tokens=256)
    text = result["choices"][0]["text"]
    questions = re.findall(r"[^.\n!?]*\?", text)
    return [q.strip() for q in questions if q.strip()][:5]

# âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜
async def stream_cbt3_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    user_input = state.question.strip()

    # ë¬´ì˜ë¯¸í•œ ì…ë ¥ ë°©ì§€
    if len(user_input) < 2 or re.fullmatch(r"[ã…‹ã…]+", user_input):
        fallback = "ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        yield fallback.encode("utf-8")
        return

    try:
        llm = load_cbt3_model(model_path)

        # ì§ˆë¬¸ ì„¸íŠ¸ê°€ ì—†ë‹¤ë©´ ìµœì´ˆ ìƒì„±
        if not state.preset_questions:
            state.preset_questions = generate_preset_questions(llm)
            state.turn = 0

        # í˜„ì¬ í„´ì˜ ì§ˆë¬¸
        if state.turn < len(state.preset_questions):
            reply = state.preset_questions[state.turn]
        else:
            reply = "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢‹ì€ ì‹¤ì²œ ê³„íšì´ ì„¸ì›Œì¡Œì–´ìš”."

        # ì „í™˜ ì²˜ë¦¬
        next_turn = state.turn + 1
        next_stage = "end" if next_turn >= 5 else "cbt3"
        next_turn = 0 if next_stage == "end" else next_turn

        if next_stage == "end":
            reply += "\n\nğŸ¯ ì‹¤ì²œ ê³„íšì„ ì˜ ì •ë¦¬í•´ì£¼ì…¨ì–´ìš”. ì´ì œ ì˜¤ëŠ˜ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í• ê²Œìš”."

        yield b"\n" + reply.encode("utf-8")

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "turn": next_turn,
            "response": reply,
            "history": state.history + [user_input, reply],
            "preset_questions": state.preset_questions
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        print(f"âš ï¸ CBT3 ì˜¤ë¥˜: {e}", flush=True)
        fallback = "ì£„ì†¡í•´ìš”. ì§€ê¸ˆì€ ì ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì´ì•¼ê¸°í•´ ì£¼ì‹œê² ì–´ìš”?"
        yield fallback.encode("utf-8")
