import os, json, multiprocessing, difflib
from typing import AsyncGenerator, Literal, List, Optional
from pydantic import BaseModel
from llama_cpp import Llama

# âœ… ëª¨ë¸ ìºì‹œ
LLM_CBT3_INSTANCE = {}

def load_cbt3_model(model_path: str) -> Llama:
    global LLM_CBT3_INSTANCE
    if model_path not in LLM_CBT3_INSTANCE:
        print("ğŸš€ CBT3 ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...", flush=True)
        NUM_THREADS = max(1, multiprocessing.cpu_count() - 1)
        LLM_CBT3_INSTANCE[model_path] = Llama(
            model_path=model_path,
            n_threads=NUM_THREADS,
            n_ctx=1500,
            n_batch=8,
            max_tokens=128,
            temperature=0.65,
            top_p=0.9,
            presence_penalty=1.0,
            frequency_penalty=0.8,
            repeat_penalty=1.1,
            n_gpu_layers=0,
            low_vram=True,
            use_mlock=False,
            verbose=False,
            chat_format="llama-3",
            stop=["<|im_end|>"]
        )
        print(f"âœ… CBT3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_path}", flush=True)
    return LLM_CBT3_INSTANCE[model_path]

# âœ… ìƒíƒœ ì •ì˜
class AgentState(BaseModel):
    stage: Literal["cbt3", "end"]
    question: str
    response: str
    history: List[str]
    turn: int
    intro_shown: bool
    awaiting_preparation_decision: bool = False
    pending_response: Optional[str] = None

# âœ… CBT3 ì‘ë‹µ í•¨ìˆ˜
async def stream_cbt3_reply(state: AgentState, model_path: str) -> AsyncGenerator[bytes, None]:
    user_input = state.question.strip()

    # âœ… ì¸íŠ¸ë¡œ ì¶œë ¥
    if not state.intro_shown:
        intro = (
            "ğŸ“˜ ì´ì œ ìš°ë¦¬ëŠ” ì‹¤ì²œ ê³„íšì„ ì„¸ì›Œë³¼ ê±°ì˜ˆìš”. ì§€ê¸ˆê¹Œì§€ ì •ë¦¬ëœ ìƒê°ì„ ë°”íƒ•ìœ¼ë¡œ, "
            "ì•ìœ¼ë¡œ ì–´ë–¤ í–‰ë™ì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆì„ì§€ í•¨ê»˜ ê³ ë¯¼í•´ë´ìš”."
        )
        yield intro.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt3",
            "turn": 0,
            "response": intro,
            "intro_shown": True,
            "awaiting_preparation_decision": False,
            "history": state.history + [intro]
        }, ensure_ascii=False).encode("utf-8")
        return

    if not user_input:
        fallback = "ë– ì˜¤ë¥´ëŠ” ì•„ì´ë””ì–´ë‚˜ ì‹œë„í•´ë³´ê³  ì‹¶ì€ ë³€í™”ê°€ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "cbt3",
            "turn": state.turn,
            "response": fallback,
            "intro_shown": True,
            "awaiting_preparation_decision": False,
            "history": state.history
        }, ensure_ascii=False).encode("utf-8")
        return

    try:
        llm = load_cbt3_model(model_path)

        system_prompt = (
            "ë„ˆëŠ” ë”°ëœ»í•˜ê³  ë…¼ë¦¬ì ì¸ ì†Œí¬ë¼í…ŒìŠ¤ ìƒë‹´ìì…ë‹ˆë‹¤.\n"
            "- ì‚¬ìš©ìê°€ ë§í•œ ê°ì •, ìƒí™©, ëª©í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì²œ ê°€ëŠ¥í•œ í–‰ë™ ê³„íšì„ ì„¸ìš°ë„ë¡ ìœ ë„í•˜ì„¸ìš”.\n"
            "- ë°˜ë“œì‹œ í•œ ë²ˆì— **í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ** í•˜ì„¸ìš”. ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ì— ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”.\n"
            "- ì§ˆë¬¸ì€ ì¡´ëŒ“ë§ë¡œ ë§ˆë¬´ë¦¬í•˜ë©°, ë‹¨ì •í•˜ì§€ ì•Šê³  ì—´ë¦° ì§ˆë¬¸ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.\n"
            "- ì‹¤ì²œ ì „ëµ, ë°©í•´ ìš”ì†Œ ëŒ€ì²˜, ìê¸° í”¼ë“œë°±, í™˜ê²½ ì„¤ì •, ê°ì • ë³€í™” ì¸ì‹ ë“± ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.\n"
            "- ê°™ì€ êµ¬ì¡°ì˜ ì§ˆë¬¸ ë°˜ë³µì€ í”¼í•˜ê³ , ë§¤ë²ˆ ìƒˆë¡œìš´ ì‹œê°ìœ¼ë¡œ ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.\n"
            "- ì˜ˆ: 'ê·¸ ë³€í™”ë¥¼ ìœ„í•´ ê°€ì¥ ë¨¼ì € ì‹œë„í•´ë³¼ ìˆ˜ ìˆëŠ” í–‰ë™ì€ ë¬´ì—‡ì¼ê¹Œìš”?'"
        )

        messages = [{"role": "system", "content": system_prompt}]
        for i in range(0, len(state.history), 2):
            messages.append({"role": "user", "content": state.history[i]})
            if i + 1 < len(state.history):
                messages.append({"role": "assistant", "content": state.history[i + 1]})
        messages.append({"role": "user", "content": user_input})

        full_response = ""
        first_token_sent = False

        for chunk in llm.create_chat_completion(messages=messages, stream=True):
            token = chunk["choices"][0]["delta"].get("content", "")
            if token:
                full_response += token
                if not first_token_sent:
                    yield b"\n"
                    first_token_sent = True
                yield token.encode("utf-8")

        reply = full_response.strip() or "ê´œì°®ì•„ìš”. ì§€ê¸ˆ ë– ì˜¤ë¥´ëŠ” ì‘ì€ ì•„ì´ë””ì–´ë¼ë„ í•¨ê»˜ ë‚˜ëˆ ë³¼ ìˆ˜ ìˆì–´ìš”."

        # âœ… ìœ ì‚¬ ì‘ë‹µ íšŒí”¼
        for past in state.history[-10:]:
            if isinstance(past, str):
                similarity = difflib.SequenceMatcher(None, reply[:30], past[:30]).ratio()
                if similarity > 0.85:
                    reply += " ì´ë²ˆì—ëŠ” ì¡°ê¸ˆ ë‹¤ë¥¸ ê°ë„ë¡œ ì§ˆë¬¸ë“œë ¸ì–´ìš”."
                    break

        # âœ… 5í„´ í›„ ì¢…ë£Œ
        next_turn = state.turn + 1
        is_ending = next_turn >= 5
        next_stage = "end" if is_ending else "cbt3"
        next_turn = 0 if is_ending else next_turn

        if is_ending:
            reply += "\n\nğŸ¯ ê³„íšì„ ì˜ ì„¸ì›Œì£¼ì…¨ì–´ìš”. ì´ì œ ì˜¤ëŠ˜ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í• ê²Œìš”."

        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": next_stage,
            "turn": next_turn,
            "response": reply,
            "intro_shown": True,
            "awaiting_preparation_decision": False,
            "history": state.history + [user_input, reply]
        }, ensure_ascii=False).encode("utf-8")

    except Exception as e:
        print(f"âš ï¸ CBT3 ì‘ë‹µ ì˜¤ë¥˜: {e}", flush=True)
        fallback = "ì£„ì†¡í•´ìš”. ë‹¤ì‹œ í•œ ë²ˆ ì´ì•¼ê¸°í•´ì£¼ì‹œê² ì–´ìš”?"
        yield fallback.encode("utf-8")
        yield b"\n---END_STAGE---\n" + json.dumps({
            "next_stage": "end",
            "turn": 0,
            "response": "âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•´ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.",
            "intro_shown": True,
            "awaiting_preparation_decision": False,
            "history": state.history
        }, ensure_ascii=False).encode("utf-8")
